---
apiVersion: sloth.slok.dev/v1
kind: PrometheusServiceLevel
metadata:
  name: apps-availability
  namespace: observability
  labels:
    app.kubernetes.io/name: sloth
spec:
  service: "apps"
  labels:
    owner: platform
  slos:
    - name: availability
      objective: 99
      description: "Availability SLO based on server span-metrics (Tempo metrics-generator)."
      timeWindow: 30d
      sli:
        events:
          errorQuery: |
            sum(
              rate(traces_spanmetrics_calls_total{span_kind="SPAN_KIND_SERVER", http_status_code=~"5..", k8s_namespace_name!~"(kube-system|flux-system|observability|cert-manager|cnpg-system|longhorn-system|network|arc-systems)"}[{{.window}}])
            )
          totalQuery: |
            sum(
              rate(traces_spanmetrics_calls_total{span_kind="SPAN_KIND_SERVER", k8s_namespace_name!~"(kube-system|flux-system|observability|cert-manager|cnpg-system|longhorn-system|network|arc-systems)"}[{{.window}}])
            )
      alerting:
        name: AppsAvailability
        labels:
          category: slo
        annotations:
          summary: "Apps availability SLO burn rate (apps)"
          description: |
            What's happening:
            - Fast burn indicates user-visible errors across app namespaces.

            Impact/risk:
            - Users are likely seeing increased 5xx and failures.

            Likely causes:
            - A major backend dependency outage
            - A bad deployment/config rollout
            - Cluster saturation (CPU/memory/IO)

            First actions:
            - Use traces/service graphs to locate the failing service path
            - Check error rates and recent deploys in the top offender namespace
            - Roll back recent changes if correlated with the start time
          runbook_url: "https://backstage.${SECRET_DOMAIN}/docs/default/component/homelab-cluster/runbooks/#apps-baseline"
          dashboard_url: "https://grafana.${SECRET_DOMAIN}/d/tempo-service-graphs/tempo-service-graphs"
        pageAlert:
          labels:
            severity: critical
        ticketAlert:
          labels:
            severity: warning
